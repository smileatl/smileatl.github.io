(window.webpackJsonp=window.webpackJsonp||[]).push([[234],{550:function(t,a,s){"use strict";s.r(a);var r=s(7),_=Object(r.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"机器学习只是点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习只是点"}},[t._v("#")]),t._v(" 机器学习只是点")]),t._v(" "),a("h2",{attrs:{id:"_1、用神经网络解决回归问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、用神经网络解决回归问题"}},[t._v("#")]),t._v(" 1、用神经网络解决回归问题")]),t._v(" "),a("h3",{attrs:{id:"_3-6-回归问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-回归问题"}},[t._v("#")]),t._v(" 3.6 回归问题")]),t._v(" "),a("h4",{attrs:{id:"_1-logistic回归不是回归算法-而是分类算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-logistic回归不是回归算法-而是分类算法"}},[t._v("#")]),t._v(" （1）logistic回归不是回归算法，而是分类算法")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021592771.png",alt:"1686021592771"}})]),t._v(" "),a("h4",{attrs:{id:"_2-准备数据-标准化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-准备数据-标准化"}},[t._v("#")]),t._v(" （2）准备数据，标准化")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021604044.png",alt:"1686021604044"}})]),t._v(" "),a("h4",{attrs:{id:"_1、代码-数据标准化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、代码-数据标准化"}},[t._v("#")]),t._v(" 1、代码：数据标准化")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021626449.png",alt:"1686021626449"}})]),t._v(" "),a("h5",{attrs:{id:"注意-用于测试数据标准化的均值和标准差都是在训练数据上计算得到的"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#注意-用于测试数据标准化的均值和标准差都是在训练数据上计算得到的"}},[t._v("#")]),t._v(" 注意：用于测试数据标准化的均值和标准差都是在训练数据上计算得到的")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021636181.png",alt:"1686021636181"}})]),t._v(" "),a("h4",{attrs:{id:"_3-训练数据数据越少-过拟合会越严重-而较小的网络可以降低过拟合"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-训练数据数据越少-过拟合会越严重-而较小的网络可以降低过拟合"}},[t._v("#")]),t._v(" （3）训练数据数据越少，过拟合会越严重，而较小的网络可以降低过拟合")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021697182.png",alt:"1686021697182"}})]),t._v(" "),a("h4",{attrs:{id:"_2、代码-模型定义"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、代码-模型定义"}},[t._v("#")]),t._v(" 2、代码：模型定义")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021705189.png",alt:"1686021705189"}})]),t._v(" "),a("h5",{attrs:{id:"注意-最后一层线性层-mse损失函数回归问题中常用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#注意-最后一层线性层-mse损失函数回归问题中常用"}},[t._v("#")]),t._v(" 注意：最后一层线性层，mse损失函数回归问题中常用")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021719090.png",alt:"1686021719090"}})]),t._v(" "),a("h4",{attrs:{id:"_4-验证集非常小-误差很大波动-验证集的划分方式可能造成验证分数上很大的方差"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-验证集非常小-误差很大波动-验证集的划分方式可能造成验证分数上很大的方差"}},[t._v("#")]),t._v(" （4）验证集非常小，误差很大波动，验证集的划分方式可能造成验证分数上很大的方差")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021726912.png",alt:"1686021726912"}})]),t._v(" "),a("h4",{attrs:{id:"_3-6-4-k折交叉验证"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-4-k折交叉验证"}},[t._v("#")]),t._v(" 3.6.4 K折交叉验证")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021735620.png",alt:"1686021735620"}})]),t._v(" "),a("h4",{attrs:{id:"_3、代码-k折验证"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、代码-k折验证"}},[t._v("#")]),t._v(" 3、代码：K折验证")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021750028.png",alt:"1686021750028"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021754042.png",alt:"1686021754042"}})]),t._v(" "),a("h5",{attrs:{id:"注意-数据集小的时候应该用k折交叉验证"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#注意-数据集小的时候应该用k折交叉验证"}},[t._v("#")]),t._v(" 注意：数据集小的时候应该用K折交叉验证")]),t._v(" "),a("h4",{attrs:{id:"_4、代码-保存每折的验证结果"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、代码-保存每折的验证结果"}},[t._v("#")]),t._v(" 4、代码：保存每折的验证结果")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021770538.png",alt:"1686021770538"}})]),t._v(" "),a("h4",{attrs:{id:"_5、代码-计算所有轮次中的-k折验证分数平均值"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5、代码-计算所有轮次中的-k折验证分数平均值"}},[t._v("#")]),t._v(" 5、代码：计算所有轮次中的 K折验证分数平均值")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021778706.png",alt:"1686021778706"}})]),t._v(" "),a("h4",{attrs:{id:"_6、代码-绘制验证分数-删除前10个数据点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6、代码-绘制验证分数-删除前10个数据点"}},[t._v("#")]),t._v(" 6、代码：绘制验证分数（删除前10个数据点）")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021790007.png",alt:"1686021790007"}})]),t._v(" "),a("h4",{attrs:{id:"_7、代码-训练最终模型-在所有训练数据上训练"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7、代码-训练最终模型-在所有训练数据上训练"}},[t._v("#")]),t._v(" 7、代码：训练最终模型，在所有训练数据上训练")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021807444.png",alt:"1686021807444"}})]),t._v(" "),a("h4",{attrs:{id:"_5-小结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-小结"}},[t._v("#")]),t._v(" （5）小结")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021822432.png",alt:"1686021822432"}})]),t._v(" "),a("ol",[a("li",[t._v("回归常用损失mse")]),t._v(" "),a("li",[t._v("不用精度，用MAE做回归指标")]),t._v(" "),a("li",[t._v("输入特征具备不同的取值范围，应该预处理")]),t._v(" "),a("li",[t._v("数据集小，使用K折验证")]),t._v(" "),a("li",[t._v("可用训练数据少，隐层要少，避免过拟合")])]),t._v(" "),a("h3",{attrs:{id:"本章小结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#本章小结"}},[t._v("#")]),t._v(" 本章小结：")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021885242.png",alt:"1686021885242"}})]),t._v(" "),a("h2",{attrs:{id:"其他"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其他"}},[t._v("#")]),t._v(" 其他")]),t._v(" "),a("h3",{attrs:{id:"embedding层"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#embedding层"}},[t._v("#")]),t._v(" Embedding层")]),t._v(" "),a("p",[a("strong",[t._v("Embeding层在某种程度上就是用来降维的")])]),t._v(" "),a("p",[t._v("embedding层，在某种程度上，就是用来降维的，降维的原理就是矩阵乘法。在卷积网络中，可以理解为特殊全连接层操作，跟1x1卷积核异曲同工")]),t._v(" "),a("p",[a("img",{attrs:{src:"/assets/1686021899530.png",alt:"1686021899530"}})]),t._v(" "),a("p",[t._v("A X B时，B的行数必须等于A的列数得出的结果为A的行数 X B的列数的一个矩阵。也就是说，假如我们有一个100W X10W的矩阵，用它乘上一个10W X 20的矩阵，我们可以把它降到100W X 20，瞬间量级降了。。。10W/20=5000倍！！！")]),t._v(" "),a("p",[t._v("这就是嵌入层的一个作用——降维。然后中间那个10W X 20的矩阵，可以理解为查询表，也可以理解为映射表，也可以理解为过度表，whatever。")]),t._v(" "),a("h3",{attrs:{id:"数据泄漏为什么会引起过拟合"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据泄漏为什么会引起过拟合"}},[t._v("#")]),t._v(" 数据泄漏为什么会引起过拟合")]),t._v(" "),a("p",[t._v("数据泄漏会导致模型在训练过程中接触到了本应该被保留的测试数据，从而使得模型在训练集上表现很好但在实际测试集上表现不佳。这会使得模型对测试数据出现过拟合现象，即过度依赖于训练数据中的噪声和异常值，而无法泛化到新的数据集上。")]),t._v(" "),a("h3",{attrs:{id:"如何用gan生成带有标签的数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#如何用gan生成带有标签的数据"}},[t._v("#")]),t._v(" 如何用GAN生成带有标签的数据")]),t._v(" "),a("p",[t._v("GAN（生成对抗网络）是一种无监督学习的方法，它可以生成与训练数据集相似的新数据。如果需要生成带有标签的数据，可以采用以下方法：")]),t._v(" "),a("ol",[a("li",[a("p",[t._v("准备带有标签的原始数据集。这些数据应该包含标签信息，以便在生成新数据时可以指定标签。")])]),t._v(" "),a("li",[a("p",[t._v("训练一个条件GAN模型（CGAN），它可以生成带有指定标签的新数据。CGAN是一种特殊的GAN，它在生成新数据时可以指定条件，例如标签信息。")])]),t._v(" "),a("li",[a("p",[t._v("在训练CGAN模型时，需要将原始数据集中的标签信息与数据一起输入模型。这样，模型可以学习如何根据标签信息生成相应的数据。")])]),t._v(" "),a("li",[a("p",[t._v("训练完成后，可以使用CGAN模型生成带有指定标签的新数据。只需输入所需的标签信息，模型就可以生成相应的数据。")])])]),t._v(" "),a("p",[t._v("需要注意的是，生成的数据可能存在一定的噪声和偏差，因此需要对生成的数据进行筛选和清洗，以保证数据的质量和可靠性。此外，还需要注意生成的数据与原始数据集的分布是否一致，以避免引入偏差和误差。")]),t._v(" "),a("p",[t._v("总之，使用CGAN生成带有标签的数据是一种有效的方法，可以生成符合特定标签要求的数据，但需要注意数据的质量和分布的一致性。")]),t._v(" "),a("h3",{attrs:{id:"怎么样才算深度学习-普通的ann算吗"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#怎么样才算深度学习-普通的ann算吗"}},[t._v("#")]),t._v(" 怎么样才算深度学习，普通的ANN算吗")]),t._v(" "),a("p",[t._v("深度学习是一种机器学习的方法，它使用多层神经网络来学习数据的特征表示。通常情况下，如果神经网络的层数较少，例如只有一层或两层，那么这种方法就不被称为深度学习。\n因此，普通的ANN（人工神经网络）通常不被认为是深度学习。ANN通常只包含一层或两层神经元，用于解决一些简单的分类或回归问题。而深度学习模型通常包含多个隐藏层，可以处理更复杂的数据和任务，例如图像识别、自然语言处理等。\n总之，深度学习通常指的是包含多个隐藏层的神经网络模型，而普通的ANN通常不被认为是深度学习。")])])}),[],!1,null,null,null);a.default=_.exports}}]);